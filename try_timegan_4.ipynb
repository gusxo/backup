{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import math\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(dir, filecnt=54):\n",
    "    data = [None]*10\n",
    "    t = np.zeros((10, filecnt), int)\n",
    "    for p in range(10):\n",
    "        d = [None]*filecnt\n",
    "        for i in range(filecnt):\n",
    "            d[i] = np.load(f\"{dir}/{p}/{i+1}.npy\")\n",
    "            t[p,i] = d[i].shape[0]\n",
    "        data[p] = d\n",
    "    return data, t\n",
    "\n",
    "def load_with_config(dir, config_path, load_nonvalid=False):\n",
    "    config_arr = np.array(pd.read_csv(f\"{config_path}\", header=None, skiprows=1))\n",
    "    data = [[] for i in range(10)]          #create empty 2d list : (10, unknown)\n",
    "    for config in config_arr:\n",
    "        #valid check\n",
    "        if (not config[2]) and (not load_nonvalid):\n",
    "            continue\n",
    "        data[int(config[0])] += [np.load(f\"{dir}/{int(config[0])}/{int(config[1])+1}.npy\")[int(config[3]):int(config[4]),:]]\n",
    "    \n",
    "    return data, config_arr\n",
    "\n",
    "def match_length(d, t:int):\n",
    "    \"\"\"\n",
    "    return\n",
    "        x : (N, times(t), 2) shape numpy array,\n",
    "        y : (N)\n",
    "    \"\"\"\n",
    "    N = sum([len(d[i]) for i in range(len(d))])\n",
    "    x = np.zeros((N, t, 2), np.float64)\n",
    "    y = np.zeros((N), np.float64)\n",
    "    target_timepoints = np.linspace(0, 1, t)\n",
    "    start_at = 0\n",
    "    for r in range(len(d)):\n",
    "        for c in range(len(d[r])):\n",
    "            origin_timepoints = np.linspace(0, 1, d[r][c].shape[0])\n",
    "            x[start_at + c, :, 0] = np.interp(target_timepoints, origin_timepoints, d[r][c][:,0])\n",
    "            x[start_at + c, :, 1] = np.interp(target_timepoints, origin_timepoints, d[r][c][:,1])\n",
    "        y[start_at:start_at + len(d[r])] = r\n",
    "        start_at += len(d[r])\n",
    "    return x, y \n",
    "\n",
    "def apply_normalize(d):\n",
    "    for r in range(len(d)):\n",
    "        for c in range(len(d[r])):\n",
    "            channels = d[r][c].shape[1]\n",
    "            min_vals = np.min(d[r][c][:, :], axis=0)\n",
    "            max_vals = np.max(d[r][c][:, :], axis=0)\n",
    "            min_max_diff = np.array([max_vals[j] - min_vals[j] for j in range(channels)])\n",
    "            factor = min_max_diff / np.max(min_max_diff)\n",
    "            for ch in range(channels):\n",
    "                d[r][c][:, ch] = ((d[r][c][:, ch] - min_vals[ch])/min_max_diff[ch])*factor[ch]\n",
    "    \n",
    "def plot_data(d):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(2,1,1)\n",
    "    gca = plt.gca()\n",
    "    gca.plot(d[:,0])\n",
    "    plt.subplot(2,1,2)\n",
    "    gca = plt.gca()\n",
    "    gca.plot(d[:,1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_data2(d, save=None, cmap_name=\"gist_rainbow\", xlim1=None, ylim1=None, xlim2=None, ylim2=None, xlim3=None, ylim3=None):\n",
    "    fig, axes = plt.subplot_mosaic(\"abbbb;acccc\", figsize=(20,4))\n",
    "    draw_gradation(d[:,0], d[:,1], axes[\"a\"],cmap_name=cmap_name, xlim=xlim1, ylim=ylim1)\n",
    "    draw_gradation(np.arange(d.shape[0]), d[:,0], axes[\"b\"],cmap_name=cmap_name, xlim=xlim2, ylim=ylim2)\n",
    "    draw_gradation(np.arange(d.shape[0]), d[:,1], axes[\"c\"],cmap_name=cmap_name, xlim=xlim3, ylim=ylim3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save)\n",
    "        plt.close()\n",
    "\n",
    "def plot_data3(origin, syn, save=None, cmap_name=\"gist_rainbow\", hightitle=\"Original\", lowtitle=\"Synthetic\"):\n",
    "    fig, axes = plt.subplot_mosaic(\"abbbb;acccc;deeee;dffff\", figsize=(20,8))\n",
    "    draw_gradation(origin[:,0], origin[:,1], axes[\"a\"],cmap_name=cmap_name, xlim=[0,1], ylim=[0,1])\n",
    "    draw_gradation(np.arange(origin.shape[0]), origin[:,0], axes[\"b\"],cmap_name=cmap_name, ylim=[0,1])\n",
    "    draw_gradation(np.arange(origin.shape[0]), origin[:,1], axes[\"c\"],cmap_name=cmap_name, ylim=[0,1])\n",
    "    draw_gradation(syn[:,0], syn[:,1], axes[\"d\"], cmap_name=cmap_name, xlim=[0,1], ylim=[0,1])\n",
    "    draw_gradation(np.arange(syn.shape[0]), syn[:,0], axes[\"e\"],cmap_name=cmap_name, ylim=[0,1])\n",
    "    draw_gradation(np.arange(syn.shape[0]), syn[:,1], axes[\"f\"],cmap_name=cmap_name, ylim=[0,1])\n",
    "    axes[\"a\"].set_title(hightitle)\n",
    "    axes[\"d\"].set_title(lowtitle)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save)\n",
    "        plt.close()\n",
    "\n",
    "def plot_data4(datas, save=None, titles=None, cmap_name=\"winter_r\"):\n",
    "    plotlen = len(datas)\n",
    "    mosaic_input = [None] * (plotlen << 1)\n",
    "    for i in range(plotlen):\n",
    "        j = i << 1\n",
    "        mosaic_input[j] = [f\"{i}a\"] + [f\"{i}x\"] * 4\n",
    "        mosaic_input[j+1] = [f\"{i}a\"] + [f\"{i}y\"] * 4\n",
    "    fig, axes = plt.subplot_mosaic(mosaic_input, figsize=(20, 4 * plotlen))\n",
    "    for i in range(plotlen):\n",
    "        if titles is not None:\n",
    "            axes[f\"{i}a\"].set_title(titles[i])\n",
    "        draw_gradation(datas[i][:,0], datas[i][:,1], axes[f\"{i}a\"], cmap_name=cmap_name, xlim=[0,1], ylim=[0,1])\n",
    "        x_indexes = np.arange(datas[i].shape[0])\n",
    "        draw_gradation(x_indexes, datas[i][:,0], axes[f\"{i}x\"], cmap_name=cmap_name, ylim=[0,1])\n",
    "        draw_gradation(x_indexes, datas[i][:,1], axes[f\"{i}y\"], cmap_name=cmap_name, ylim=[0,1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, config = load_with_config(\"/home/user/workspace/research/eye-writing/self_data/\", \"/home/user/workspace/research/eye-writing/load_data_config.csv\", load_nonvalid=False)\n",
    "apply_normalize(data)\n",
    "# plot_data2(data[9][53], xlim1=[0, 1], ylim1=[0,1], ylim2=[0,1], ylim3=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_data = data\n",
    "# save_dir = \"/home/user/img\"\n",
    "# for r in range(len(target_data)):\n",
    "#     target_pattern_config = config[config[:,0] == r, :]\n",
    "#     indexes = target_pattern_config[target_pattern_config[:,2]==1, 1]\n",
    "#     print(f\"pattern {r}, length = {len(target_data[r])}, indexes len = {indexes.shape}\")\n",
    "#     for c in range(len(target_data[r])):\n",
    "#         plot_data2(target_data[r][c], save=f\"{save_dir}/Pattern_{r}_Index_{indexes[c]}.png\", cmap_name=\"gist_rainbow\", xlim1=[0, 1], ylim1=[0,1], ylim2=[0,1], ylim3=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(505, 200, 2)\n",
      "(505,)\n",
      "(51, 200, 2)\n"
     ]
    }
   ],
   "source": [
    "x, y = match_length(data, 200)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "pattern = 0\n",
    "x = x[y==pattern]\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_data = x\n",
    "# save_dir = \"/home/user/img\"\n",
    "# for i in range(len(target_data)):\n",
    "#     plot_data2(target_data[i], f\"{save_dir}/{i}.png\", \"gist_rainbow\",  xlim1=[0, 1], ylim1=[0,1], ylim2=[0,1], ylim3=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tqdm import trange\n",
    "\n",
    "def get_model(input_shape, output_units, rnn_units, layer_cnt):\n",
    "    inputs = keras.layers.Input(input_shape)\n",
    "    x = inputs\n",
    "    for i in range(layer_cnt):\n",
    "        x = keras.layers.GRU(rnn_units, return_sequences=True)(x)\n",
    "    outputs = keras.layers.Dense(output_units, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def timegan_init(time_series_len, features, rnn_units, rnn_layers):\n",
    "    input_shape = (time_series_len, features)\n",
    "    latent_code_shape = (time_series_len, rnn_units)\n",
    "    embedder = get_model(input_shape, rnn_units, rnn_units, rnn_layers)\n",
    "    generator = get_model(input_shape, rnn_units, rnn_units, rnn_layers)\n",
    "    supervisor = get_model(latent_code_shape, rnn_units, rnn_units, rnn_layers)\n",
    "    recovery = get_model(latent_code_shape, features, rnn_units, rnn_layers)\n",
    "    discriminator = get_model(latent_code_shape, 1, rnn_units, rnn_layers-1)\n",
    "    return embedder, generator, supervisor, recovery, discriminator\n",
    "\n",
    "def timegan_export_generator(timegan_tuple):\n",
    "    _, generator, supervisor, recovery, _ = timegan_tuple\n",
    "    syn_gen_input = keras.Input(generator.input_shape[1:])\n",
    "    syn_gen_output = generator(syn_gen_input)\n",
    "    syn_gen_output = supervisor(syn_gen_output)\n",
    "    syn_gen_output = recovery(syn_gen_output)\n",
    "    return keras.Model(syn_gen_input, syn_gen_output)\n",
    "    \n",
    "def timegan_train(x, timegan_tuple, epochs, batch_size, learning_rate):\n",
    "    embedder, generator, supervisor, recovery, discriminator = timegan_tuple\n",
    "\n",
    "    #random_vector에 쓰이는 np.random.uniform()이 float32기 때문에 타입을 맞춰줌(안맞추면 에러남)\n",
    "    x = x.astype(np.float32)\n",
    "    mini_batch = lambda x, batch_size : x[np.random.permutation(x.shape[0])[:batch_size]]\n",
    "\n",
    "    #loss & optimizer\n",
    "    mse = keras.losses.MeanSquaredError()\n",
    "    bce = keras.losses.BinaryCrossentropy()\n",
    "    opt_autoencoder = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    opt_supervisor = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    opt_generator = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    opt_embedder = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    opt_discriminator = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    #train autoencoder\n",
    "    print(f\"train autoencoder\")\n",
    "    for _ in trange(epochs):\n",
    "        batch = mini_batch(x, batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_true = embedder(batch)\n",
    "            y_true = recovery(y_true)\n",
    "            loss = 10 * tf.sqrt(mse(y_true, batch))\n",
    "        var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "        gradients = tape.gradient(loss, var_list)\n",
    "        opt_autoencoder.apply_gradients(zip(gradients, var_list))\n",
    "\n",
    "    #train supervisor\n",
    "    print(f\"train supervisor\") \n",
    "    for _ in trange(epochs):\n",
    "        batch = mini_batch(x, batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_true = embedder(batch)\n",
    "            y_pred = supervisor(y_true)\n",
    "            loss = mse(y_true[:, 1:, :], y_pred[:, :-1, :])\n",
    "                        \n",
    "        var_list = generator.trainable_variables + supervisor.trainable_variables\n",
    "        gradients = tape.gradient(loss, var_list)\n",
    "        #이거 안하면 TF2 WARNING : Gradients do not exist for variables 발생\n",
    "        #var_list에 계산과정에 없던 generator있어서 그런거 같은데, 정확한 원인은 모르겠음.\n",
    "        apply_grads = [(grad, var) for (grad, var) in zip(gradients, var_list) if grad is not None]\n",
    "        opt_supervisor.apply_gradients(apply_grads)\n",
    "\n",
    "    #joint train\n",
    "    print(f\"joint train\")\n",
    "    for _ in trange(epochs):\n",
    "        for __ in range(2):\n",
    "            batch = mini_batch(x, batch_size)\n",
    "            random_vector = np.random.uniform(size=batch.shape)\n",
    "            #train generator\n",
    "            with tf.GradientTape() as tape:\n",
    "                #supervised loss\n",
    "                y_true = embedder(batch)\n",
    "                y_pred = supervisor(y_true)\n",
    "                supervised_loss = mse(y_true[:, 1:, :], y_pred[:, :-1, :])\n",
    "\n",
    "                #unsupervised loss\n",
    "                y_true = tf.ones((batch_size, x.shape[1], 1))\n",
    "                y_pred = generator(random_vector)\n",
    "                y_pred = supervisor(y_pred)\n",
    "                y_pred = discriminator(y_pred)\n",
    "                unsupervised_loss = bce(y_true, y_pred)\n",
    "\n",
    "                #unsupervised loss - E\n",
    "                y_true = tf.ones((batch_size, x.shape[1], 1))\n",
    "                y_pred = generator(random_vector)\n",
    "                y_pred = discriminator(y_pred)\n",
    "                unsupervised_loss_e = bce(y_true, y_pred)\n",
    "\n",
    "                #moment loss\n",
    "                y_true = batch\n",
    "                y_pred = generator(random_vector)\n",
    "                y_pred = supervisor(y_pred)\n",
    "                y_pred = recovery(y_pred)\n",
    "                y_true_mean, y_true_var = tf.nn.moments(y_true, axes=[0])\n",
    "                y_pred_mean, y_pred_var = tf.nn.moments(y_pred, axes=[0])\n",
    "                v1 = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n",
    "                v2 = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))\n",
    "                moment_loss = v1 + v2\n",
    "\n",
    "                loss = supervised_loss + 100 * tf.sqrt(unsupervised_loss) + unsupervised_loss_e + 100 * moment_loss\n",
    "\n",
    "            var_list = generator.trainable_variables + supervisor.trainable_variables\n",
    "            gradients = tape.gradient(loss, var_list)\n",
    "            opt_generator.apply_gradients(zip(gradients, var_list))\n",
    "\n",
    "            #train embedder\n",
    "            with tf.GradientTape() as tape:\n",
    "                #supervised loss\n",
    "                y_true = embedder(batch)\n",
    "                y_pred = supervisor(y_true)\n",
    "                supervised_loss = mse(y_true[:, 1:, :], y_pred[:, :-1, :])\n",
    "\n",
    "                #reconstruction loss\n",
    "                y_true = embedder(batch)\n",
    "                y_true = recovery(y_true)\n",
    "                y_pred = batch\n",
    "                reconstruction_loss = 10 * tf.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "                loss = reconstruction_loss + 0.1 * supervised_loss\n",
    "\n",
    "            var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "            gradients = tape.gradient(loss, var_list)\n",
    "            opt_embedder.apply_gradients(zip(gradients, var_list))\n",
    "\n",
    "        batch = mini_batch(x, batch_size)\n",
    "        random_vector = np.random.uniform(size=batch.shape)\n",
    "        #train discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            #loss on FN\n",
    "            y_true = tf.ones((batch_size, x.shape[1], 1))\n",
    "            y_pred = embedder(batch)\n",
    "            y_pred = discriminator(y_pred)\n",
    "            loss_on_FN = bce(y_true, y_pred)\n",
    "\n",
    "            #loss on FP\n",
    "            y_true = tf.zeros((batch_size, x.shape[1], 1))\n",
    "            y_pred = generator(random_vector)\n",
    "            y_pred = supervisor(y_pred)\n",
    "            y_pred = discriminator(y_pred)\n",
    "            loss_on_FP = bce(y_true, y_pred)\n",
    "\n",
    "            #loss on FP - E\n",
    "            y_true = tf.zeros((batch_size, x.shape[1], 1))\n",
    "            y_pred = generator(random_vector)\n",
    "            y_pred = discriminator(y_pred)\n",
    "            loss_on_FP_E = bce(y_true, y_pred)           \n",
    "\n",
    "            loss = loss_on_FN + loss_on_FP + loss_on_FP_E\n",
    "\n",
    "        var_list = discriminator.trainable_variables\n",
    "        gradients = tape.gradient(loss, var_list)\n",
    "        opt_discriminator.apply_gradients(zip(gradients, var_list))\n",
    "\n",
    "    return embedder, generator, supervisor, recovery, discriminator\n",
    "\n",
    "def generator_save(syn_gen, save_path):\n",
    "    syn_gen.save_weights(save_path)\n",
    "\n",
    "def generator_load(save_path, time_series_len, features, rnn_units, rnn_layers):\n",
    "    timegan = timegan_init(time_series_len, features, rnn_units, rnn_layers)\n",
    "    syn_gen = timegan_export_generator(timegan)\n",
    "    syn_gen.load_weights(save_path)\n",
    "    return syn_gen\n",
    "\n",
    "def generator_gen(syn_gen, generate_cnt):\n",
    "    syn = syn_gen.predict(np.random.uniform(size=(generate_cnt, syn_gen.input_shape[1], syn_gen.input_shape[2])))\n",
    "    return syn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train autoencoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:11<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train supervisor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:55<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [1:02:33<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "rnn_units = 24\n",
    "rnn_layers = 3\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "epoch = 2000\n",
    "save_name = f\"/home/user/backup/synckpt_test_{pattern}/\"\n",
    "import os\n",
    "\n",
    "z = np.random.uniform(size=(batch_size, x.shape[1], x.shape[2]))\n",
    "\n",
    "if os.path.isdir(save_name):\n",
    "    print(f\"체크포인트 있음, 불러옴.\")\n",
    "    syn_gen = generator_load(save_name, x.shape[1], x.shape[2], rnn_units, rnn_layers)\n",
    "else:\n",
    "    timegan_tuple = timegan_init(x.shape[1], x.shape[2], rnn_units, rnn_layers)\n",
    "    timegan_tuple = timegan_train(x, timegan_tuple, epochs=epoch, batch_size=batch_size, learning_rate=lr)\n",
    "    syn_gen = timegan_export_generator(timegan_tuple)\n",
    "    # generator_save(syn_gen, save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synth_data = generator_gen(syn_gen, batch_size)\n",
    "# synth_data.shape\n",
    "e, g, s, r ,d = timegan_tuple\n",
    "# synth_data = e(x)\n",
    "# synth_data = r(synth_data)\n",
    "# synth_data = g(z)\n",
    "# synth_data = s(synth_data)\n",
    "\n",
    "e_l = e(x)\n",
    "e_s_l = s(e_l)\n",
    "e_l_r = r(e_l)\n",
    "e_s_l_r = r(e_s_l)\n",
    "g_l = g(z)\n",
    "g_s_l = s(g_l)\n",
    "g_l_r = r(g_l)\n",
    "g_s_l_r = r(g_s_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# plot_data3(x[i], synth_data[i], None, \"winter_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_data = synth_data\n",
    "# save_dir = f\"/home/user/syn{pattern}\"\n",
    "# for i in range(len(synth_data)):\n",
    "#     plot_data2(synth_data[i], f\"{save_dir}/{i}.png\", \"gist_rainbow\",   xlim1=[0, 1], ylim1=[0,1], ylim2=[0,1], ylim3=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f\"/home/user/custom_compare_images_{pattern}\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob(f'{save_dir}/*.png')\n",
    "for f in files:\n",
    "    try:\n",
    "        os.remove(f)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s : %s\" % (f, e.strerror))\n",
    "\n",
    "# for i in range(len(synth_data)):\n",
    "#     plot_data3(x[i], synth_data[i], f\"{save_dir}/{i}.png\", \"winter_r\")\n",
    "\n",
    "for i in range(batch_size):\n",
    "    plot_data4(datas=[g_l[i], g_s_l[i], g_l_r[i], g_s_l_r[i]],\n",
    "     save=f\"{save_dir}/{i}.png\", \n",
    "     titles=[\"z -> latent code\", \"z -> latent code(with supervisor)\", \"z -> reconstructions\", \"z -> reconstructions(with supervisor)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
